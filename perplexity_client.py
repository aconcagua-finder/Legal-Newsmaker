import requests  # –î–æ–±–∞–≤–ª–µ–Ω –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–π –∏–º–ø–æ—Ä—Ç
import json
import re
from datetime import datetime, timedelta
from typing import Optional, Tuple, List
from loguru import logger

import config
from prompts import (
    get_perplexity_system_prompt,
    get_perplexity_news_prompt,
    PromptConfig
)
from validation import is_content_fresh, get_date_feedback_for_next_prompt


class PerplexityClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Perplexity API (–º–æ–¥–µ–ª—å Sonar Deep Research)"""
    
    def __init__(self, web_config=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Perplexity
        
        Args:
            web_config: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
        """
        self.api_key = config.PERPLEXITY_API_KEY
        self.api_url = config.PERPLEXITY_API_URL
        self.timeout = config.REQUEST_TIMEOUT
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –≤–µ–±-–∫–æ–Ω—Ñ–∏–≥–∞ –µ—Å–ª–∏ –æ–Ω–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã
        if web_config and 'api_models' in web_config:
            perplexity_config = web_config['api_models'].get('perplexity', {})
            self.model = perplexity_config.get('model', config.PERPLEXITY_MODEL)
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ max_tokens –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏
            max_tokens_limits = perplexity_config.get('max_tokens_limits', {})
            if self.model in max_tokens_limits:
                max_allowed = max_tokens_limits[self.model]
                requested = perplexity_config.get('max_tokens', config.PERPLEXITY_MAX_TOKENS)
                self.max_tokens = min(requested, max_allowed)
                if requested > max_allowed:
                    logger.warning(f"–ó–∞–ø—Ä–æ—à–µ–Ω–æ {requested} —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è {self.model}, –Ω–æ –ª–∏–º–∏—Ç {max_allowed}. –ò—Å–ø–æ–ª—å–∑—É–µ–º {self.max_tokens}")
            else:
                self.max_tokens = perplexity_config.get('max_tokens', config.PERPLEXITY_MAX_TOKENS)
            
            self.temperature = perplexity_config.get('temperature', 0.7)
            self.top_p = perplexity_config.get('top_p', 0.9)
            self.presence_penalty = perplexity_config.get('presence_penalty', 0.0)
            self.frequency_penalty = perplexity_config.get('frequency_penalty', 0.0)
            self.return_citations = perplexity_config.get('return_citations', True)
            self.return_related_questions = perplexity_config.get('return_related_questions', False)
            self.search_domain_filter = perplexity_config.get('search_domain_filter', [])
            
            # –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
            self.search_recency_filter = perplexity_config.get('search_recency_filter', None)
            self.search_depth = perplexity_config.get('search_depth', 'high')
            self.search_after_date_filter = perplexity_config.get('search_after_date_filter', None)
            self.search_before_date_filter = perplexity_config.get('search_before_date_filter', None)
            self.web_search_options = perplexity_config.get('web_search_options', {
                'search_context_size': 'high',
                'enable_deep_search': True
            })
        else:
            # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            self.model = config.PERPLEXITY_MODEL
            self.max_tokens = config.PERPLEXITY_MAX_TOKENS
            self.temperature = 0.7
            self.top_p = 0.9
            self.presence_penalty = 0.0
            self.frequency_penalty = 0.0
            self.return_citations = True
            self.return_related_questions = False
            self.search_domain_filter = []
            self.search_recency_filter = None
            self.search_depth = 'high'
            self.search_after_date_filter = None
            self.search_before_date_filter = None
            self.web_search_options = {
                'search_context_size': 'high',
                'enable_deep_search': True
            }
        
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
    
    def _clean_deep_research_content(self, content: str) -> str:
        """
        –û—á–∏—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç —Ç–µ–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π Deep Research
        
        Args:
            content: –°—ã—Ä–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç Deep Research API
            
        Returns:
            str: –û—á–∏—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –±–µ–∑ —Ç–µ–≥–æ–≤ <think> –∏ </think>
        """
        # –£–¥–∞–ª—è–µ–º –±–ª–æ–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π <think>...</think>
        cleaned = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
        
        # –£–¥–∞–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Ç–µ–≥–∏
        cleaned = re.sub(r'</?think>', '', cleaned)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        cleaned = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned)
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        return cleaned.strip()
    
    
    
    def _extract_sources_from_content(self, content: str) -> Tuple[str, List[str]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ AI –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å –Ω–æ–º–µ—Ä–∞–º–∏ + —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫
        
        Args:
            content: –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç AI
            
        Returns:
            Tuple[str, List[str]]: (—Ç–µ–∫—Å—Ç —Å –Ω–æ–º–µ—Ä–∞–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –ø–æ –ø–æ—Ä—è–¥–∫—É)
        """
        sources = []
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–Ω—É—é —á–∞—Å—Ç—å –∏ —Å–µ–∫—Ü–∏—é —Å—Å—ã–ª–æ–∫
        lines = content.split('\n')
        main_content_lines = []
        sources_section_started = False
        
        for line in lines:
            # –ò—â–µ–º —Å—Ç—Ä–æ–∫—É "–ò–°–¢–û–ß–ù–ò–ö–ò:" –∏ –Ω–∞—á–∏–Ω–∞–µ–º —Å–µ–∫—Ü–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            if line.strip().upper() == '–ò–°–¢–û–ß–ù–ò–ö–ò:' or line.strip().upper().startswith('–ò–°–¢–û–ß–ù–ò–ö'):
                sources_section_started = True
                # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º —ç—Ç—É —Å—Ç—Ä–æ–∫—É –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
                continue
                
            # –ò—â–µ–º —Å—Ç—Ä–æ–∫—É —Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º (–Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç: "üîó –ò—Å—Ç–æ—á–Ω–∏–∫: https://...")
            if line.strip().startswith('üîó'):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ "üîó –ò—Å—Ç–æ—á–Ω–∏–∫: https://—Å—Å—ã–ª–∫–∞ [1]"
                url_match = re.search(r'https?://[^\s\[\]]+', line)
                if url_match:
                    source_url = url_match.group().rstrip('.,;:')
                    sources.append(source_url)
                # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç - —É–±–∏—Ä–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                continue
                
            # –¢–∞–∫–∂–µ –∏—â–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–µ–∫—Ü–∏–∏ —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            if re.match(r'^–°—Å—ã–ª–∫–∏:\s*$', line.strip(), re.IGNORECASE):
                sources_section_started = True
                continue
            
            if sources_section_started:
                # –í —Å–µ–∫—Ü–∏–∏ —Å—Å—ã–ª–æ–∫ –∏—â–µ–º —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞ [–Ω–æ–º–µ—Ä] —Å—Å—ã–ª–∫–∞
                source_match = re.match(r'^\[(\d+)\]\s*(https?://[^\s]+)', line.strip())
                if source_match:
                    source_num = int(source_match.group(1))
                    source_url = source_match.group(2).rstrip('.,;:')
                    
                    # –†–∞—Å—à–∏—Ä—è–µ–º —Å–ø–∏—Å–æ–∫ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    while len(sources) < source_num:
                        sources.append('')
                    sources[source_num - 1] = source_url
            else:
                # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç - —Å–æ—Ö—Ä–∞–Ω—è–µ–º
                main_content_lines.append(line)
        
        # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ sources
        sources = [s for s in sources if s]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
        main_content = '\n'.join(main_content_lines).strip()
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å–µ–∫—Ü–∏–∏, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ —Ç–µ–∫—Å—Ç–µ
        if not sources:
            url_pattern = r'https?://[^\s\)\]>]+'
            sources = re.findall(url_pattern, content)
            sources = list(set(sources))
            sources = [source.rstrip('.,;:') for source in sources]
            
            # –ï—Å–ª–∏ URL –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –Ω–æ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if not sources and '[1]' in main_content:
                # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç–æ–≤
                bill_match = re.search(r'N\s*(\d+-\d+)', main_content)
                if bill_match:
                    bill_number = bill_match.group(1)
                    sources.append(f'https://sozd.duma.gov.ru/bill/{bill_number}')
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫
                    sources.append('https://sozd.duma.gov.ru/')
                    
                logger.warning("–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –æ—Ç–≤–µ—Ç–µ, –¥–æ–±–∞–≤–ª–µ–Ω—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏")
        
        # –ï—Å–ª–∏ –≤–æ–æ–±—â–µ –Ω–∏–∫–∞–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –¥–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
        if not sources:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–æ–º–µ—Ä –∑–∞–∫–æ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ
            law_match = re.search(r'‚Ññ\s*(\d+-–§–ó)', main_content)
            if law_match:
                law_number = law_match.group(1)
                sources.append(f'https://sozd.duma.gov.ru/bill/{law_number}')
            else:
                # –î–µ—Ñ–æ–ª—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–∞ —Å–∞–π—Ç –∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç–æ–≤
                sources.append('https://sozd.duma.gov.ru/')
            
            logger.warning("–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É—é –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫")
        
        logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        if sources:
            for i, source in enumerate(sources, 1):
                logger.debug(f"–ò—Å—Ç–æ—á–Ω–∏–∫ [{i}]: {source}")
        
        return main_content, sources
    
    def get_legal_updates(self) -> Optional[dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö –∑–∞ –≤—á–µ—Ä–∞—à–Ω–∏–π –¥–µ–Ω—å
        
        Returns:
            dict: {'content': str, 'sources': List[str]} –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            prompt = get_perplexity_news_prompt()
            from prompts import get_yesterday_date
            yesterday_date = get_yesterday_date()
            
            logger.info(f"–ó–∞–ø—Ä–∞—à–∏–≤–∞—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö –∑–∞ {yesterday_date}")
            
            payload = {
                "model": self.model,
                "messages": [
                    {
                        "role": "system",
                        "content": get_perplexity_system_prompt()
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "max_tokens": self.max_tokens,
                "temperature": self.temperature,
                "top_p": self.top_p,
                "presence_penalty": self.presence_penalty,
                "frequency_penalty": self.frequency_penalty,
                "return_citations": self.return_citations,
                "return_related_questions": self.return_related_questions
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä –¥–æ–º–µ–Ω–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å
            if self.search_domain_filter:
                payload["search_domain_filter"] = self.search_domain_filter
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –µ—Å–ª–∏ –∑–∞–¥–∞–Ω—ã
            if self.search_recency_filter:
                payload["search_recency_filter"] = self.search_recency_filter
                logger.debug(f"–ü—Ä–∏–º–µ–Ω—è—é —Ñ–∏–ª—å—Ç—Ä —Å–≤–µ–∂–µ—Å—Ç–∏: {self.search_recency_filter}")
            
            if self.search_after_date_filter:
                payload["search_after_date_filter"] = self.search_after_date_filter
                logger.debug(f"–ò—â—É –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ—Å–ª–µ: {self.search_after_date_filter}")
            
            if self.search_before_date_filter:
                payload["search_before_date_filter"] = self.search_before_date_filter
                logger.debug(f"–ò—â—É –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ: {self.search_before_date_filter}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –æ–ø—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –¥–ª—è deep research
            if 'deep-research' in self.model.lower() and self.web_search_options:
                if self.web_search_options.get('search_context_size'):
                    payload["search_context_size"] = self.web_search_options['search_context_size']
                if self.web_search_options.get('enable_deep_search'):
                    payload["enable_deep_search"] = self.web_search_options['enable_deep_search']
                logger.info(f"Deep Research —Ä–µ–∂–∏–º —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: {self.web_search_options.get('search_context_size', 'default')}")
            
            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=payload,
                timeout=self.timeout
            )
            
            response.raise_for_status()
            
            data = response.json()
            raw_content = data['choices'][0]['message']['content']
            
            logger.info("–£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Perplexity API")
            logger.debug(f"–û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤): {raw_content[:200]}...")
            
            # –û—á–∏—â–∞–µ–º –æ—Ç —Ç–µ–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π Deep Research
            cleaned_content = self._clean_deep_research_content(raw_content)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            content, sources = self._extract_sources_from_content(cleaned_content)
            
            # –£–¥–∞–ª—è–µ–º –ª—é–±—ã–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –º–∞—Ä–∫–µ—Ä—ã —Å—Å—ã–ª–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞
            content = re.sub(r'\s*\[\d+\]', '', content)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏
            is_fresh, freshness_reason = is_content_fresh(content, max_age_days=3)
            logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏: {freshness_reason}")
            
            if not is_fresh:
                logger.warning("‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å —É—Å—Ç–∞—Ä–µ–ª–∞, –ø–æ–ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑ —Å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏")
                # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
                # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å, –Ω–æ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º
            
            return {
                'content': content,
                'sources': sources
            }
            
        except requests.exceptions.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Perplexity API: {e}")
            return None
            
        except KeyError as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç API: {e}")
            return None
            
        except Exception as e:
            logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å API: {e}")
            return None
    
    def test_connection(self) -> bool:
        """
        –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ API
        
        Returns:
            bool: True –µ—Å–ª–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
        """
        try:
            test_payload = {
                "model": self.model,
                "messages": [
                    {
                        "role": "user",
                        "content": "–ü—Ä–∏–≤–µ—Ç! –≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å."
                    }
                ],
                "max_tokens": 50,
                "temperature": self.temperature,
                "return_citations": False  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∞
            }
            
            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=test_payload,
                timeout=10
            )
            
            response.raise_for_status()
            logger.info("–¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Perplexity API –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
            return False 